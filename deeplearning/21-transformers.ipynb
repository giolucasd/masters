{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdc200a",
   "metadata": {},
   "source": [
    "### https://huggingface.co/transformers has pipelines, pretrained models, tokenizers etc for several tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c169752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#is GPU available?\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "#defining device where to to the computation\n",
    "#device = torch.device(0) if gpu else torch.device('cpu')\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d3196",
   "metadata": {},
   "source": [
    "### Sentiment analysis: recall we used it in our introductory lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2940f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df08e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: NEGATIVE Score: 0.9992689490318298\n",
      "Sentiment: POSITIVE Score: 0.999553382396698\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"I hate washing dishes\")[0]\n",
    "print(\"Sentiment: {} Score: {}\".format(result['label'],result['score']))\n",
    "\n",
    "result = classifier(\"I love visiting Paris\")[0]\n",
    "print(\"Sentiment: {} Score: {}\".format(result['label'],result['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330311d9",
   "metadata": {},
   "source": [
    "### You can also fine tune any pretrained model. For instance, let's fine tune a model for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63f1b6",
   "metadata": {},
   "source": [
    "### Load a dataset for fine-tuning. You can get imdbs.csv from\n",
    "### https://drive.google.com/uc?id=11_M4ootuT7I1G0RlihcC0cA3Elqotlc-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1606e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files='./imdbs.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d736418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test\n",
    "\n",
    "dataset   = dataset.train_test_split(test_size=0.1)\n",
    "train_set = dataset['train']\n",
    "test_set  = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a15d2",
   "metadata": {},
   "source": [
    "### Load the tokenizer and preprocess the training and test sets with the tokenizer -- it already converts tokens into ids and sets attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17424231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e664553fa141d9b375cf1a93f1fc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0af6d58e56e4f13ba7796293d00e7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# preprocess the dataset \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_set = train_set.map(tokenize_function, batched=True)\n",
    "test_set  = test_set.map(tokenize_function, batched=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad40f2",
   "metadata": {},
   "source": [
    "### Load the model for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6fce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e29c0",
   "metadata": {},
   "source": [
    "### Since we want to report the accuracy of the model, we can add the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929add51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31962/3043712826.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782ae46",
   "metadata": {},
   "source": [
    "### Now set training parameters and arguments, and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492db2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba8fb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# set training parameters and arguments\n",
    "\n",
    "batch_size = 8\n",
    "epochs     = 20\n",
    "warmup_steps = 100\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs',    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a47f4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6061e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:02, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.704127</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.671948</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648823</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.630535</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.611535</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.558160</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.487023</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575655</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.666871</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.228140</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.804531</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.700778</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.604429</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.036858</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.031435</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.024484</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.23227124214172362, metrics={'train_runtime': 63.1974, 'train_samples_per_second': 28.482, 'train_steps_per_second': 1.899, 'total_flos': 473599899648000.0, 'train_loss': 0.23227124214172362, 'epoch': 20.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7fedae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.005834068171679974,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_runtime': 0.1345,\n",
       " 'eval_samples_per_second': 74.332,\n",
       " 'eval_steps_per_second': 7.433,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate \n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d97a4e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "inputs  = tokenizer('High tech companies are growing up', return_tensors=\"pt\").to(device=0)\n",
    "labels  = torch.tensor([1]).unsqueeze(0).to(device=0)\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss    = outputs.loss\n",
    "logits  = outputs.logits\n",
    "answer  = torch.argmax(logits)\n",
    "if (answer == 0):\n",
    "    print(\"Sentiment: NEGATIVE\")\n",
    "else:\n",
    "    print(\"Sentiment: POSITIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad7091",
   "metadata": {},
   "source": [
    "### Extractive Question Answering: the task of extracting an answer from a text given a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c08ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a4e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: a wide variety of agents\n",
      "Score: 0.4355\n",
      "Start: 176 End: 200\n",
      "Answer: must detect a wide variety of agents\n",
      "Score: 0.058\n",
      "Start: 164 End: 200\n",
      "Answer: a system of many biological structures and processes within an organism that protects against diseases\n",
      "Score: 0.4359\n",
      "Start: 21 End: 123\n"
     ]
    }
   ],
   "source": [
    "context = \"The immune system is a system of many biological structures and processes \\\n",
    "within an organism that protects against diseases. To function properly the immune system \\\n",
    "must detect a wide variety of agents, called pathogens.\"\n",
    "\n",
    "result = question_answerer(question=\"What are pathogens?\", context=context)\n",
    "print(\"Answer: {}\".format(result['answer']))\n",
    "print(\"Score: {}\".format(round(result['score'], 4)))\n",
    "print(\"Start: {} End: {}\".format(result['start'],result['end']))\n",
    "\n",
    "result = question_answerer(question=\"How does the immune system work?\", context=context)\n",
    "print(\"Answer: {}\".format(result['answer']))\n",
    "print(\"Score: {}\".format(round(result['score'], 4)))\n",
    "print(\"Start: {} End: {}\".format(result['start'],result['end']))\n",
    "\n",
    "result = question_answerer(question=\"What is the immune system?\", context=context)\n",
    "print(\"Answer: {}\".format(result['answer']))\n",
    "print(\"Score: {}\".format(round(result['score'], 4)))\n",
    "print(\"Start: {} End: {}\".format(result['start'],result['end']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc250881",
   "metadata": {},
   "source": [
    "### You may also use pretrained models already fine-tuned in some dataset (e.g., SQUAD -- Stanford Question-Answering Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c3d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87957378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned model\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a078d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load its fine-tuned tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94534fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add BERT tags to process question and context\n",
    "\n",
    "question = \"[CLS]\" + \"What are pathogens?\" + \"[SEP]\"\n",
    "context  = context + \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f530f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get their tokens, combine and convert them into input_ids\n",
    "question_tokens = tokenizer.tokenize(question)\n",
    "context_tokens  = tokenizer.tokenize(context)\n",
    "all_tokens      = question_tokens + context_tokens\n",
    "input_ids       = tokenizer.convert_tokens_to_ids(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8337789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define segment_ids with zeroes for question tokens and ones for context tokens\n",
    "\n",
    "segment_ids = [0] * len(question_tokens)\n",
    "segment_ids = segment_ids + [1] * len(context_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a9c7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input and segment ids to tensors and feed them into the model \n",
    "# to obtain the start and end scores\n",
    "\n",
    "input_ids              = torch.tensor([input_ids])\n",
    "segment_ids            = torch.tensor([segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f1a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(input_ids, token_type_ids = segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61c57d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:What are pathogens?\n",
      "\n",
      "Answer: agents.\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(result.start_logits)\n",
    "answer_end   = torch.argmax(result.end_logits)\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(all_tokens[answer_start:answer_end+1])\n",
    "    print(\"\\nQuestion:{}\".format(question[5:-5]))\n",
    "    print(\"\\nAnswer: {}.\".format(answer))\n",
    "else:\n",
    "    print(\"I could not find an answer to your question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2f5df",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d93d631a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cefbd88e9da4859b8e9df911340498e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "572098dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'It is a strong idea that the government should not interfere with the free market.\\n\\n# Answer\\nThe statement \"It is a strong idea that the government should'}]\n"
     ]
    }
   ],
   "source": [
    "text = text_generator(\"It is a strong idea\", truncation=True, max_length=35, do_sample=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b033bc2",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00bef386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4b2cba1a674a28802c313ab11d269a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8534d2cd1c4120a49d091193dd7cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a63c5c2ed649e683289fa004794e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7529a4b4a06940bdb4a87c67fc9361b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff113270e834158aa90903b37059e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dde10200df492397f334253ee69639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf3d446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ORG', 'score': 0.9988896, 'index': 1, 'word': 'IBM', 'start': 0, 'end': 3}\n",
      "{'entity': 'B-MISC', 'score': 0.81814283, 'index': 3, 'word': 'Eagle', 'start': 15, 'end': 20}\n",
      "{'entity': 'B-LOC', 'score': 0.99930835, 'index': 5, 'word': 'USA', 'start': 24, 'end': 27}\n"
     ]
    }
   ],
   "source": [
    "text = \"IBM introduces Eagle in USA -- the first processor to surpass 100 qubits.\"\n",
    "result = ner_pipe(text)\n",
    "for entity in result:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bc112",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca823b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4011a1a6004086aa3682bc03dd075c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056ec8c9d0e344daa1a7847e9d411d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e86f8e899a44459e09861e16aef43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6db0c687bae48a7948ac84d08bd4795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba799c8fc9e240aca3eb70ba500c92ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600a7b3451d7432d8ebc4b660a9dc816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c776c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"IBM has unveiled an advanced quantum processor that is part of an effort \\\n",
    "to build super-fast computers. These machines could revolutionise computing, harnessing \\\n",
    "the strange world of quantum physics to solve problems beyond reach for even the most \\\n",
    "advanced classical ones. But the hurdles in building practical, large-scale versions \\\n",
    "have kept quantum computers confined to the lab. The new chip has 127 qubits, \\\n",
    "twice as many as the previous IBM processor. Qubits (quantum bits) are the most basic \\\n",
    "units of information in a quantum computer. The company called its new Eagle processor \\\n",
    "a key milestone on the path towards practical quantum computation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48cc5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = summarizer(text, max_length=50, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9615d31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM has unveiled an advanced quantum processor that is part of an effort to build super-fast computers. These machines could revolutionise computing, harnessing the strange world of quantum physics to solve problems beyond reach for even the most advanced classical\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b3fc7",
   "metadata": {},
   "source": [
    "### Translation: For example, from English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17941cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879b75e1c21049e3bccaf94222835dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebce712d28a647a69375f973c776da66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30d1145501645b4923c3d7bfaa7ea2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7903105c6b99469384e07b9905d14c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abefdce3e9964f479efb4136a2ca8dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60456d609e24decbe14c2e997d0f530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", model = \"Faizyhugging/English_to_French\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd943148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le livre est sur la table.\n"
     ]
    }
   ],
   "source": [
    "result = translator(\"The book is on the table.\")\n",
    "print(result[0]['translation_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
