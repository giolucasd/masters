{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified from Text Analytics with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim is a super fast library to create feature vectors (embeddings) from text  -- https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, let's load a pretrained deep learning model: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word2vecA = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model aims at assigning nearby feature vectors to similar words, allowing linear operations such as:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n",
      "[('king', 0.8449392318725586), ('queen', 0.7300518155097961)]\n"
     ]
    }
   ],
   "source": [
    "# W1 and W2 should be embeddings near each other: W1 ~= W2\n",
    "W1 = word2vecA[\"king\"] - word2vecA[\"man\"] + word2vecA[\"woman\"]\n",
    "W2 = word2vecA[\"queen\"]\n",
    "# You can find that by \n",
    "print(word2vecA.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1))\n",
    "# Or you can also verify that by searching the two closest words to W1\n",
    "print(word2vecA.most_similar(positive=[W1], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second, let's import required packages and use a corpus to train our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 30103\n",
      "\n",
      "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "\n",
      "Processed line: god said let firmament midst waters let divide waters waters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from   nltk.corpus import gutenberg, stopwords\n",
    "from   string      import punctuation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "wpt        = nltk.WordPunctTokenizer()\n",
    "stop_words = stopwords.words('english') # define stopwords\n",
    "\n",
    "# Create a standard function to normalize documents. It will be used to normalize your own corpus later\n",
    "\n",
    "def normalize_document(doc): \n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "bible = gutenberg.sents('bible-kjv.txt') \n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "# convert to lower case and remove punctuation and digits\n",
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "# apply your standard function for document normalization, and then \n",
    "# eliminate sentences with less than two tokens\n",
    "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
    "\n",
    "print('Total lines:', len(bible))\n",
    "print('\\nSample line:', bible[10])\n",
    "print('\\nProcessed line:', norm_bible[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your Word2Vec model on the selected corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_bible]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size   = 300  # embedding dimensionality  \n",
    "window_context = 30   # context window size: 15 words around the central word                                                                                    \n",
    "min_word_count = 3    # minimum word count: consider the word when it appears at least 3 times in the corpus      \n",
    "num_cores      = 12   # number of processors in your machine to speed up process if you have cython \n",
    "\n",
    "word2vecB      = Word2Vec(sentences=tokenized_corpus, vector_size=feature_size, window=window_context, \\\n",
    "                          min_count=min_word_count, epochs=50, workers=num_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'god': ['lord', 'sworn', 'rebellion', 'covenant', 'glory'],\n",
       " 'jesus': ['peter', 'john', 'impotent', 'apostles', 'repentance'],\n",
       " 'noah': ['shem', 'japheth', 'ham', 'methuselah', 'milcah'],\n",
       " 'egypt': ['pharaoh', 'egyptians', 'bondage', 'flowing', 'rid'],\n",
       " 'john': ['james', 'baptist', 'peter', 'galilee', 'baptism'],\n",
       " 'gospel': ['christ', 'preach', 'faith', 'hope', 'godly'],\n",
       " 'moses': ['aaron', 'congregation', 'joshua', 'sinai', 'gerizim'],\n",
       " 'famine': ['pestilence', 'blasting', 'sojourn', 'mildew', 'noisome']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary with some similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in word2vecB.wv.most_similar([search_term], topn=5)]\n",
    "                for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same operations now produce different results, due to the change of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wife', 0.30001696944236755)]\n",
      "[('woman', 0.6494314074516296), ('king', 0.5991543531417847)]\n"
     ]
    }
   ],
   "source": [
    "# W1 and W2 should be embeddings near each other: W1 ~= W2\n",
    "W1 = word2vecB.wv[\"king\"] - word2vecB.wv[\"man\"] + word2vecB.wv[\"woman\"]\n",
    "W2 = word2vecB.wv[\"queen\"]\n",
    "# You can find that by \n",
    "print(word2vecB.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1))\n",
    "# Or you can also verify that by searching the two closest words to W1\n",
    "print(word2vecB.wv.most_similar(positive=[W1], topn=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = word2vecB.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, max_iter=10000, perplexity=10)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             Document Category\n",
      "0                                      The sky is blue and beautiful.  weather\n",
      "1                                   Love this blue and beautiful sky!  weather\n",
      "2                        The quick brown fox jumps over the lazy dog.  animals\n",
      "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
      "4                         I love green eggs, ham, sausages and bacon!     food\n",
      "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
      "6            The sky is very blue and the sky is very beautiful today  weather\n",
      "7                         The dog is lazy but the brown fox is quick!  animals\n"
     ]
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "print(corpus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sky blue beautiful' 'love blue beautiful sky'\n",
      " 'quick brown fox jumps lazy dog'\n",
      " 'kings breakfast sausages ham bacon eggs toast beans'\n",
      " 'love green eggs ham sausages bacon' 'brown fox quick blue dog lazy'\n",
      " 'sky blue sky beautiful today' 'dog lazy brown fox quick']\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "print(norm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec model on your sample corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size   = 10  # embedding dimensionality  \n",
    "window_context = 3   # context window size: 1 word around the central word                                                                                    \n",
    "min_word_count = 1   # minimum word count: consider the word when it appears at least 1 time in the corpus      \n",
    "num_cores      = 12  # number of processors in your machine to speed up process if you have cython \n",
    "\n",
    "word2vecC      = Word2Vec(sentences=tokenized_corpus, vector_size=feature_size, window=window_context, \\\n",
    "                          min_count=min_word_count, epochs=100, workers=num_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': ['blue', 'bacon', 'sausages'],\n",
       " 'fox': ['quick', 'today', 'love'],\n",
       " 'breakfast': ['green', 'kings', 'love']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary with some similar words\n",
    "similar_words = {search_term: [item[0] for item in word2vecC.wv.most_similar([search_term], topn=3)]\n",
    "                for search_term in ['sky', 'fox', 'breakfast']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "words = word2vecC.wv.index_to_key\n",
    "wvs   = word2vecC.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00650653,  0.00165769,  0.05251156,  0.09033262, -0.09172861,\n",
       "       -0.07161501,  0.06707988,  0.09047023, -0.05170376, -0.03739343],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecC.wv['sky']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build framework for getting document (sentence) level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim # Average of the word vectors of a given sentence\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "# Compute the embedding of each sentence in a corpus as the average of its word embeddings    \n",
    "def Doc2Vec(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    #print(vocabulary)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) \\\n",
    "                for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003058</td>\n",
       "      <td>-0.008641</td>\n",
       "      <td>0.035086</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>-0.053435</td>\n",
       "      <td>-0.036183</td>\n",
       "      <td>0.060932</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>-0.045775</td>\n",
       "      <td>-0.059238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003382</td>\n",
       "      <td>-0.018292</td>\n",
       "      <td>0.047592</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>-0.022776</td>\n",
       "      <td>-0.019782</td>\n",
       "      <td>0.034021</td>\n",
       "      <td>0.022211</td>\n",
       "      <td>-0.039022</td>\n",
       "      <td>-0.027430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.026250</td>\n",
       "      <td>0.043631</td>\n",
       "      <td>-0.026681</td>\n",
       "      <td>-0.041218</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.018762</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>-0.007740</td>\n",
       "      <td>0.025239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006139</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>0.013477</td>\n",
       "      <td>0.019014</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.013165</td>\n",
       "      <td>0.018590</td>\n",
       "      <td>-0.005417</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>0.013170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001664</td>\n",
       "      <td>-0.029173</td>\n",
       "      <td>0.029612</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>0.037718</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.023164</td>\n",
       "      <td>-0.020708</td>\n",
       "      <td>0.036433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>-0.026354</td>\n",
       "      <td>-0.021669</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>-0.011599</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.024727</td>\n",
       "      <td>-0.034063</td>\n",
       "      <td>-0.001417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.001069</td>\n",
       "      <td>-0.008166</td>\n",
       "      <td>0.036277</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>-0.055612</td>\n",
       "      <td>-0.030766</td>\n",
       "      <td>0.061074</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>-0.057121</td>\n",
       "      <td>-0.033951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.014345</td>\n",
       "      <td>0.045987</td>\n",
       "      <td>-0.022765</td>\n",
       "      <td>-0.039259</td>\n",
       "      <td>0.037476</td>\n",
       "      <td>-0.010353</td>\n",
       "      <td>0.006809</td>\n",
       "      <td>0.027462</td>\n",
       "      <td>-0.024049</td>\n",
       "      <td>0.016994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.003058 -0.008641  0.035086  0.027984 -0.053435 -0.036183  0.060932   \n",
       "1  0.003382 -0.018292  0.047592 -0.003435 -0.022776 -0.019782  0.034021   \n",
       "2 -0.026250  0.043631 -0.026681 -0.041218  0.037346  0.000333  0.018762   \n",
       "3 -0.006139 -0.025693  0.013477  0.019014  0.023917  0.013165  0.018590   \n",
       "4  0.001664 -0.029173  0.029612  0.024028  0.037718  0.032967  0.019836   \n",
       "5  0.000229  0.035499 -0.026354 -0.021669  0.023457 -0.011599  0.010943   \n",
       "6 -0.001069 -0.008166  0.036277  0.019190 -0.055612 -0.030766  0.061074   \n",
       "7 -0.014345  0.045987 -0.022765 -0.039259  0.037476 -0.010353  0.006809   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.014549 -0.045775 -0.059238  \n",
       "1  0.022211 -0.039022 -0.027430  \n",
       "2  0.013306 -0.007740  0.025239  \n",
       "3 -0.005417  0.010433  0.013170  \n",
       "4  0.023164 -0.020708  0.036433  \n",
       "5  0.024727 -0.034063 -0.001417  \n",
       "6  0.022239 -0.057121 -0.033951  \n",
       "7  0.027462 -0.024049  0.016994  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_feature_array = Doc2Vec(corpus=tokenized_corpus, model=word2vecC, num_features=feature_size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document  \\\n",
       "0                                      The sky is blue and beautiful.   \n",
       "1                                   Love this blue and beautiful sky!   \n",
       "2                        The quick brown fox jumps over the lazy dog.   \n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans   \n",
       "4                         I love green eggs, ham, sausages and bacon!   \n",
       "5                    The brown fox is quick and the blue dog is lazy!   \n",
       "6            The sky is very blue and the sky is very beautiful today   \n",
       "7                         The dog is lazy but the brown fox is quick!   \n",
       "\n",
       "  Category  ClusterLabel  \n",
       "0  weather             1  \n",
       "1  weather             1  \n",
       "2  animals             2  \n",
       "3     food             0  \n",
       "4     food             0  \n",
       "5  animals             2  \n",
       "6  weather             1  \n",
       "7  animals             2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(w2v_feature_array)\n",
    "cluster_labels = ap.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the PCA projection of the sentences in each cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pcs = pca.fit_transform(w2v_feature_array)\n",
    "labels = ap.labels_\n",
    "categories = list(corpus_df['Category'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    color = 'orange' if label == 0 else 'blue' if label == 1 else 'green'\n",
    "    annotation_label = categories[i]\n",
    "    x, y = pcs[i]\n",
    "    plt.scatter(x, y, c=color, edgecolors='k')\n",
    "    plt.annotate(annotation_label, xy=(x+1e-4, y+1e-3), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, load a vocabulary with word embeddings created by GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.12.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "Total of word vectors: 20000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# make sure you have a GloVe pretrained model for an english vocabulary by downloading en_vectors_web_md   \n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") # load the pipeline with the pretrained model\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total of word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [ -7.2681     -0.85717     5.8105      1.9771      8.8147     -5.8579\n",
      "   3.7143      3.585       4.7987     -4.4251      1.7461     -3.7296\n",
      "  -5.1407     -1.0792     -2.5555      3.0755      5.0141      5.8525\n",
      "   7.3378     -2.7689     -5.1641     -1.9879      2.9782      2.1024\n",
      "   4.4306      0.84355    -6.8742     -4.2949     -0.17294     3.6074\n",
      "   0.84379     0.33419    -4.8147      0.035683  -13.721      -4.6528\n",
      "  -1.4021      0.48342     1.2549     -4.0644      3.3278     -0.2159\n",
      "  -5.1786      3.536      -3.1575     -3.5273     -3.6753      1.5863\n",
      "  -8.1594     -3.4657      1.5262      4.8135     -3.8428     -3.9082\n",
      "   0.67549    -0.35787    -1.7806      3.5284     -0.051114   -0.9715\n",
      "  -0.90553    -1.557       1.2038      4.7708      0.98561    -2.3186\n",
      "  -7.4899     -9.5389      8.5572      2.742      -3.627       2.7456\n",
      "  -6.9574     -1.719      -2.9145      1.1838      3.7864      2.0413\n",
      "  -3.5808      1.4319      0.20528    -0.7064     -5.3556     -2.5911\n",
      "   4.4922      1.6574      3.9794     -4.356      -2.7266      1.9581\n",
      "  -3.4842     -3.9674      3.269       0.66683     3.9837     -6.5997\n",
      "   4.163       8.0338      0.38102     8.2656      0.97061    -5.0807\n",
      "   4.9522      7.5018      3.8305     -3.3233      4.9126      0.24189\n",
      "   3.8218     -3.9717      2.4691     13.721      -8.9664     10.61\n",
      "   0.69425   -11.082      -5.6883      2.3287      1.6451      3.6006\n",
      "   0.12588    -6.1956     11.455       5.6682     -0.50251    -0.98515\n",
      "   0.088902   -4.0213      3.6134     -9.0936    -14.555      -2.5591\n",
      "   4.0959     -0.35929     1.0219      3.9402      0.80495    -3.6023\n",
      "   2.6394     -0.15258    -2.6182     -0.26268    -2.161       2.395\n",
      "   6.8842      3.6034      1.8058      2.4528      4.4088     -1.0598\n",
      "   6.4964      5.9196     -1.0261     -1.7013     -4.4151      4.3043\n",
      "  -1.7138     -4.669      -0.55212     5.3995      1.8311     -0.3582\n",
      "  -0.36578    -2.8578     -6.4639     -3.2155      0.67083    -1.28\n",
      "   1.2782      0.78274     0.19839    -1.4163      2.1184      1.5021\n",
      "  -1.8212      1.6629      4.0354     -4.4648     -3.4897     -2.5765\n",
      "  -3.6317     -0.041619    0.4866      2.0712     -1.9166     -3.4045\n",
      "  -7.6609     -2.194      -0.0023919   0.849       1.3921     -5.783\n",
      "   4.4739      1.0642      5.7864      3.4643     -5.9169     -2.6925\n",
      "  -0.11271    -6.0462      3.9285     -3.0423     -0.069939    0.22826\n",
      "   8.0214      2.2098    -11.049       0.076001   -1.597       0.20524\n",
      "   2.8063      3.5245     -3.93       -0.97995     4.0248      1.8447\n",
      "  -2.0452      1.1419     -0.446      -0.95551    -1.0224      5.9224\n",
      "  -6.1688     -0.8384     -7.9102     -0.089575   -0.27741     4.2703\n",
      "   4.0212     -0.11166     2.5119     -5.9635     -1.232       0.28199\n",
      "  -4.1062     -0.62923    -0.5242      2.5213     -3.5094      6.4333\n",
      "   7.9466     -3.3883      5.2535      0.094524   -3.3336      5.9621\n",
      "  -1.0794     -6.085      -3.6071     -0.38496     7.6137     -9.1081\n",
      "  -6.0037     -2.4735     -0.6505     -6.3021      8.5783      0.1725\n",
      "   4.3631     -9.3439      0.20984     0.769      10.763       0.44598\n",
      "  -3.6584     -3.0992     -3.8868      4.3337     -5.8037     -1.1337\n",
      "  -6.1562      0.3182     -1.0612     -1.4809      6.0373      0.46015\n",
      "  -1.553      -1.0562      0.58618     3.4431      4.5542     -3.1881\n",
      "  -1.5832      3.0859      1.3061     -8.0091      7.7996     -5.0644\n",
      "   8.8719      0.72337    -1.235       1.6209      7.8994     10.741\n",
      "   0.81158     9.0156     -1.5913     -5.3166      0.35032    -2.885    ]\n",
      "green [-1.9339     0.31487   -4.3951     1.9247    -2.048     -5.7121\n",
      " -0.9562     2.7927    -1.3071     3.6604     6.233      0.40894\n",
      " -5.199     -3.2189     0.11595    1.0566    -2.5734     0.80832\n",
      "  5.1219    -1.155     -0.79585    1.8941     0.86538   -4.6471\n",
      " -2.7567    -2.678     -4.3315    -5.6574     2.5075     0.51175\n",
      " -4.4489    -1.9456     3.2416    -5.4982     0.87518   -0.84995\n",
      "  0.19437    4.369     -0.76414   -2.1313     5.2023    -2.2315\n",
      " -0.54212   -0.82075   -1.8244     7.4969     1.4303    -2.0703\n",
      " -4.0535     2.0932     0.37029    1.5441    -5.9293    -1.2217\n",
      "  0.53055    0.44775    1.8206     1.353      1.709      0.82163\n",
      "  1.9833    -1.7029     4.0128    -1.267     -2.4839     1.4809\n",
      "  0.64627   -7.851      1.7999     0.027256   0.74528    1.8567\n",
      "  0.72432    3.8561     2.5639     0.54397   -3.8264    -2.163\n",
      "  1.1602     2.2584    -0.8464    -0.46227    0.095909   2.5676\n",
      "  1.5718    -1.2725     1.1667     1.0352    -3.3454     2.0119\n",
      " -0.10433    0.55956    3.3005    -1.3821     0.46118   -2.2161\n",
      "  6.5293    -1.2095     2.3292     0.79599   -2.6463    -3.8801\n",
      "  3.4803     1.6597    -0.55412    4.2473    -1.5661    -1.0731\n",
      " -2.92       1.4412    -0.26055   -0.60597    2.2155     1.4566\n",
      " -2.6577     2.4713     2.3426    -9.2572     1.1058    -2.6171\n",
      " -3.5128     0.089063   0.70053    1.7643    -2.3501    -4.5125\n",
      " -2.0775    -0.24759    8.6766    -0.14041   -3.7827     0.75835\n",
      "  2.8528    -1.9713     0.45791   -2.2949    -5.5814    -4.8628\n",
      "  2.3797     3.4038    -1.4492     2.0983    -2.4242    -2.0022\n",
      " -2.1206    -6.0328    -3.7112     0.46569    5.2319     2.2868\n",
      " -0.65811    5.1747    -0.22368   -0.082336  -0.62164    0.16812\n",
      "  5.0091    -3.9224     2.9451     2.2521    -3.6781    -3.7058\n",
      "  1.1999    -1.7125    -0.17065    0.85354   -2.89       3.6739\n",
      "  1.5582     0.30095   -4.8507     1.0238     1.1045    -1.5548\n",
      " -2.9357     0.17267    0.98352   -0.050785  -4.4522    -0.33714\n",
      " -5.654      1.0711    -3.0877    -1.8951     0.60552   -1.0736\n",
      "  3.1882     1.7731     0.4086    -0.81856    2.717      1.0602\n",
      "  1.2175    -0.24953    1.5228     2.3948    -5.3563     1.3449\n",
      "  0.0054722 -1.7795     1.7951    -3.1578     0.97231   -3.0541\n",
      "  1.7054    -0.25269   -4.3012     2.3389     2.1225    -0.50549\n",
      "  5.9937    -0.22442    0.305      0.95165    2.7841     2.6324\n",
      "  5.8594    -1.4159    -4.905      2.4374     0.11655   -0.21432\n",
      " -2.3441    -1.9198    -1.1124    -5.2248    -1.9957     2.6563\n",
      "  0.21374    5.0364     5.614     -0.80226    1.8022     2.2421\n",
      " -2.8311    -0.25667   -6.8799     1.7983    -1.4061    -2.2025\n",
      "  4.0006    -3.3632    -2.2811    -0.34608   -3.0402     0.88747\n",
      " -1.5734    -1.122      1.3794    -2.0003    -1.0576     2.535\n",
      " -2.5219     0.40955    1.9064    -3.4186    -2.0789     7.4245\n",
      " -4.0004     1.0816     0.12302    2.3958    -0.8206     2.2748\n",
      "  0.372      0.67044   -4.7422    -2.2908    -5.4391     2.2508\n",
      "  2.2963     0.63337    2.9864    -1.3047    -0.79148    0.59416\n",
      "  3.0042     1.7317     1.2124     2.4705    -1.3708     4.7587\n",
      "  2.1316    -1.9626     3.3927    -2.6717    -0.95951   -1.9339\n",
      " -1.7778     3.6615    -2.7951     0.43909    2.7408    -2.1808\n",
      " -0.93622    1.7161     4.2103    -0.4414    -3.9935    -0.57955  ]\n",
      "frog [ 1.0258    1.4698    1.2748   -6.01      0.46523  -0.74243  -3.7125\n",
      " -0.27701   0.6955    5.2781    2.831    -2.3511    3.7929    3.1249\n",
      "  2.3439   -4.2838   -0.10871   1.2361    3.8146   -4.0382    1.7993\n",
      "  1.6935    0.45288  -3.9446    2.7754   -2.4043   -0.95622  -0.63344\n",
      "  4.1845    3.3493   -4.9989   -0.96089   1.8568    0.34936   1.6861\n",
      " -2.0498    1.3089   -0.94749   4.2096   -5.226     1.1977   -0.062547\n",
      "  1.356     1.4038    2.6408    1.5928    2.6488    0.58255  -4.7756\n",
      " -0.32723   2.0029    7.6926   -1.4262    1.3991   -0.97878  -1.1169\n",
      "  0.35985   0.96784  -2.0557    0.41393   1.1737   -0.90461   3.1241\n",
      "  3.4651   -2.0677    0.15348  -1.4759    1.2573    0.22136  -0.56222\n",
      "  0.64254  -0.69267   1.5075    0.41354   0.66168  -3.125    -3.1246\n",
      "  0.98155   1.859    -1.1314    0.5199   -1.484    -2.4532    0.97423\n",
      " -1.2431   -4.2974    3.9008    2.9374   -2.3141   -1.7377    0.54506\n",
      " -3.196     1.6368   -2.8755    1.4617    0.73403   0.9422    1.9371\n",
      "  0.79541  -1.1297   -3.3403    1.4173   -0.23634  -1.2255   -3.3945\n",
      " -0.11389  -1.3209    5.9457   -2.1862    2.7687   -3.3806    5.5487\n",
      "  0.93794  -1.8771    0.571    -0.03472   3.6243   -1.2812    1.5612\n",
      " -1.327     0.91061   1.3535   -8.0599    1.9368   -5.9034   -1.5217\n",
      "  2.94     -0.8125    4.8892    1.1059   -2.1237    1.1749    1.9236\n",
      " -2.3557    2.0838    2.5933   -1.9402   -1.5728   -4.7646   -1.9522\n",
      " -0.8658   -1.3728   -0.50261  -1.0259    0.81643  -2.8282   -1.1116\n",
      "  5.0081   -0.67198   0.35018   1.9286    4.721    -0.11383  -4.53\n",
      "  0.12605  -1.0364    3.171     4.0633    3.6261    0.8993    0.76241\n",
      "  1.0829   -0.35438   0.28183  -3.6398   -3.2757   -0.99004   4.2413\n",
      " -0.34748   2.0119    2.3054    4.3861   -1.0408    0.054537 -1.139\n",
      " -1.5034   -0.016019 -4.0797   -1.3716    0.77514  -0.19588  -2.8465\n",
      " -1.1246    3.5572   -1.6223    0.13613   5.9867   -3.7977    0.91754\n",
      " -4.9829    2.5571   -0.078225 -0.98406  -3.6639    0.17936  -0.15818\n",
      " -5.1231    1.6914   -3.215     0.48227  -1.9324    2.9396    1.9223\n",
      "  6.3014   -0.054238 -1.0792   -0.29715  -1.9381   -0.4921   -2.9649\n",
      "  0.21493   2.0822   -0.049046  0.61162   1.491    -0.69453   5.3732\n",
      "  1.498    -2.2108    0.58379   1.469    -2.7473    0.57883   2.2863\n",
      " -0.50044   0.302    -3.8588   -3.286     4.6109    3.7935   -0.75183\n",
      " -0.29856   3.6527    3.748    -3.5558   -1.6502   -2.3357    1.8031\n",
      "  0.4764    0.47208   2.5512    0.035865  1.2839    1.591    -0.51568\n",
      " -0.35371   3.5831    2.086    -1.4145   -1.4746   -4.0637    0.65871\n",
      " -0.66715   3.5743   -2.7076   -1.8912    2.1127    5.8136    0.55356\n",
      " -0.10211   0.87761   1.0928   -0.28926   0.66295  -3.0493   -1.0279\n",
      " -7.6605   -1.9839   -0.045374  3.99      3.7543   -1.2056    2.4245\n",
      " -1.1247    0.93888  -0.18604  -0.2691   -1.668    -3.3189    3.0297\n",
      " -3.5634   -0.59235   2.858    -1.4954    4.7213    1.5899   -0.22131\n",
      "  0.05221  -0.64339   2.1958    4.8535    0.8496    7.0404    0.12385\n",
      " -1.0745    0.56041   3.444    -0.91566  -3.1573    0.2274  ]\n",
      "jumped [-0.1656     0.66098   -0.65131   -0.31666    0.46001    0.52796\n",
      "  0.76502    7.0745     2.9037     0.49214   -3.6252     3.6184\n",
      "  1.0105     0.26956    0.17052    2.0453    -1.3066     0.7066\n",
      "  1.53       0.11041    1.8009    -3.2782     0.48623   -6.0832\n",
      " -1.4165    -0.58222    0.74538   -2.6593     4.9368     0.64189\n",
      "  1.2573    -2.8184    -0.55255   -1.4593    -3.3147    -4.0618\n",
      "  0.8812    -1.8511     2.9134     0.30923   -3.0523     0.0065259\n",
      "  4.1627    -1.518     -0.37781    0.8994     2.5842    -0.26727\n",
      " -0.5538     1.1531     2.9662     1.5619    -1.1782     2.2242\n",
      "  1.1236     1.4736     0.39106   -1.9037     2.1007     3.3096\n",
      "  0.94399   -0.85459   -0.39108   -1.0914    -1.5045    -0.7182\n",
      " -1.9528     3.6451    -0.3778     1.6411     1.5671    -0.19632\n",
      "  1.711     -0.79732    0.34476    0.72099    2.4006     2.2237\n",
      " -0.18997   -0.77337   -2.4771    -1.1156     2.5741     3.9023\n",
      "  1.7651    -2.1082     3.1394    -1.6105    -2.4322     0.32745\n",
      "  0.4142     1.1685    -3.1654    -0.13502    0.55385    3.3342\n",
      " -0.53466   -5.3442     5.3236    -0.9159     0.10842   -0.15117\n",
      "  1.7138     0.65338   -2.9111     1.2848    -1.3931     1.36\n",
      " -2.7317    -1.109     -2.1342    -2.349     -1.68      -3.2179\n",
      " -0.61629    2.6787     0.79586    0.042995  -2.3869     0.66672\n",
      "  0.8713    -0.61404   -1.6143     1.8226    -0.54402    2.958\n",
      " -0.652      2.0858     0.4579     2.8248     1.9568    -3.1184\n",
      "  1.175     -0.58959   -0.14124   -0.80521   -0.98413   -1.6571\n",
      "  1.8324    -2.1597    -0.8812    -1.4988    -1.9573     1.9461\n",
      " -2.0666     0.23946   -3.1639    -0.30019   -0.97988   -0.12833\n",
      " -1.1917    -2.0074     2.2959     1.0603    -1.195     -1.533\n",
      "  2.0106     1.5818    -0.51949    0.27972    2.7041    -1.1561\n",
      "  0.06774   -2.8215    -1.4074     3.1191     0.56115   -1.2547\n",
      "  0.16026    1.4168    -0.90592    3.0813     1.0293    -2.4491\n",
      "  0.19843   -1.2461    -3.0591    -3.7716    -1.3982    -0.045896\n",
      " -0.88803   -0.95414   -0.015728   6.3599    -3.2925    -3.3773\n",
      "  2.0126    -0.66801   -0.76053   -5.0419     1.4814     0.26395\n",
      " -1.4385    -2.3549     0.23822   -5.3993     0.7349    -0.41622\n",
      " -2.7188     3.9503     1.7741    -0.36649    0.51727   -4.5378\n",
      " -0.91272    1.3796     1.1728     0.29814   -0.85477    0.0022141\n",
      " -3.9664     0.52996   -3.3002    -2.2607     0.70257   -1.6746\n",
      "  3.8488     2.5768    -3.5987    -3.2347    -4.5011    -0.82032\n",
      " -0.59345    1.4713    -1.6163     2.2285     1.7629     2.9534\n",
      " -0.073858  -1.176     -1.2954    -1.077      0.82681    2.0072\n",
      " -2.132      1.0332    -0.47892    1.1509     2.5261    -0.30839\n",
      "  1.4008     0.17254   -2.0535    -0.67229    5.703     -1.5519\n",
      " -3.6084     1.5617    -0.31937   -1.3785    -3.351     -1.7947\n",
      " -1.4206     0.41226   -0.28986    0.65358    0.39207   -0.28119\n",
      "  1.5241     1.6256     1.6459    -0.28988   -1.5845     0.028665\n",
      " -0.62221   -5.7106    -3.0305    -2.3852     0.35447    1.8781\n",
      "  1.1677    -0.94608   -3.3916    -1.7085    -2.9703    -1.8861\n",
      " -0.29554   -2.557      2.556      0.13164    1.5407     3.2136\n",
      " -0.77551   -2.6535     2.0299     2.3046    -3.9631    -3.6056\n",
      " -4.2614     3.5154    -4.4705     0.58224   -0.23441    0.59004\n",
      " -0.95134    1.6571    -2.534      3.3461     1.4725    -0.54679  ]\n",
      "the [ -5.1043      2.3496      3.2472      2.8424     11.459      -2.4137\n",
      "   0.51057     7.0312      3.6459      0.62332    13.633       4.5813\n",
      " -10.584       1.263       0.63362     7.4645      6.1468      0.39474\n",
      "   1.4378     -4.154       2.         -3.8488      0.73414     2.1209\n",
      "   2.1068      1.8713     -7.8175     -4.4352      0.22135     3.9262\n",
      "   2.8473      2.0265     -1.8189     -9.2866     -8.2191     -1.7172\n",
      "  -1.7196      3.9313      2.5888      0.82825     1.3177      1.1566\n",
      "  -0.5268     -0.53276    -1.2835     -0.24743    -3.7231     -0.45196\n",
      "  -3.3093     -1.2523     -3.0767      8.5522     -1.6251     -5.5941\n",
      "   0.53161     0.55142    -0.50327     1.8156     -2.9011     -1.8576\n",
      "   1.4638     -5.4034     -0.81768    -2.0847      1.6838      1.6817\n",
      "  -6.3836     -3.218       7.2152      3.5279     -2.3648      0.40313\n",
      "  -5.7476     -1.0917     -0.11058     2.7065     -4.5004      5.5528\n",
      "  -7.4396     -1.8884     -5.0159      0.78553    -0.042467    3.2823\n",
      "   5.8371      2.8223     -4.7819     -4.0965      0.38425     1.3066\n",
      "  -1.5229      0.43565     3.8789     -4.8557      5.8081     -0.77726\n",
      "   4.3211      4.6135      4.2278      5.8454      4.0931      0.76313\n",
      "   8.2298      3.25        1.1713      4.9618      3.8678     -4.8282\n",
      "  -0.9828     -3.0029      2.0229      4.2098     -7.2674      5.8495\n",
      "   1.173      -1.5662      1.4491      1.4227      0.12867     4.0703\n",
      "   0.49251    -4.4845      4.0733      3.0339     -3.384      -5.2706\n",
      "   3.3339     -7.2218      5.6753     -4.5924    -11.599      -1.0203\n",
      "   5.8945      1.7762     -1.0363      2.535       1.4402     -3.1572\n",
      "   2.4663     -4.5771     -7.4574     -0.69813    -1.9212      6.5128\n",
      "   6.4388      6.4317     -6.9971      1.8715      6.2365     -0.25091\n",
      "  -1.7363      9.5132      1.4306     -2.5112      0.40763     3.7422\n",
      "   0.88706    -4.8168     -0.74616    -0.078736   -0.79536    -6.0894\n",
      "   2.7413      2.1358     -2.7505     -1.1987     -6.4908      1.3424\n",
      "   1.4893     -1.8716      2.8296     -5.308       5.1784     -0.18339\n",
      "   0.3525      1.8829      0.69355    -4.1332     -2.212      -0.28912\n",
      "  -5.2159     -3.3565     -0.26745     3.9148     -2.7248      0.37187\n",
      "  -6.4571     -0.092446    2.0403      0.55114     1.1552     -1.9635\n",
      "   7.6531     -3.4696      4.7709      1.0621     -6.7008     -1.9478\n",
      "  -2.0715     -0.52312     1.4389     -5.0093      0.43909    -7.2692\n",
      "   8.7121     -0.76601    -8.1995      1.4594     -0.26537     3.5587\n",
      "   5.4031      3.7896     -9.2572      3.4277      2.5215      4.1383\n",
      "   0.18763    -4.2169      0.40743    -1.18       -5.2174      8.7247\n",
      "  -2.5949      1.4874     -7.056      -1.6384      0.0029549   6.3286\n",
      "   1.491       0.44328     6.1122     -8.074      -1.7362      6.3054\n",
      "   2.6566     -0.65135    -0.96859     0.50822    -1.1677     -0.41731\n",
      "   1.4866      1.3182      1.3838      0.62296     1.3744      5.9588\n",
      "  -5.2347     -2.6428      0.95187    -0.53128     2.2787    -10.626\n",
      " -12.064      -0.016146    2.7706     -4.3249      6.7169     -0.29383\n",
      "   1.1804     -1.6421      3.7709     12.545       9.6831      3.1599\n",
      "   3.0962     -4.538      -0.56059     4.379      -7.2785      0.27534\n",
      "   0.010549   -0.29399     0.8385     -3.7402      3.6582      0.30861\n",
      "   0.13038    -5.0155     -0.74336     3.2874      0.10577     3.7003\n",
      "   2.4759      3.9932      2.7957     -4.1682      3.0442      1.2622\n",
      "   0.10991    -0.39761     0.13023     1.3929      0.33822     7.01\n",
      "  -1.6916      3.6008     -1.2658     -7.6875     -2.5128      0.69342  ]\n",
      "black [-5.7389     0.20553   -5.1586     4.8624     1.9055    -0.5865\n",
      "  3.0969     8.5575    -0.34645    2.0944     7.8004    -0.95294\n",
      " -4.4777     0.0087888  0.69051    0.87519   -2.9681     1.7952\n",
      "  4.7009     1.925     -2.6309     2.781      1.862     -8.0394\n",
      "  1.517     -1.6037    -2.2405    -4.1247     1.5101    -2.7155\n",
      " -4.1862     2.4029    -2.5677    -2.3151    -3.2056    -0.77968\n",
      "  0.40944    0.72921   -0.30551   -2.7241     0.091477  -3.1607\n",
      " -2.7118     1.0823     1.3733     4.1502    -2.635     -1.449\n",
      " -1.8512     7.0928    -3.7438     3.9153    -1.3732    -0.87538\n",
      " -0.021586   1.9179     5.0346     0.82027   -0.22343    1.4842\n",
      " -0.055134  -3.1091    -1.3702    -0.073583  -0.50914   -1.4873\n",
      " -0.3857    -7.1196     1.4618     4.0417     1.7108    -2.322\n",
      " -0.54857    2.3024    -0.76952    0.29886   -2.1519     1.3609\n",
      "  0.30078    0.37637   -2.0314    -0.3808     1.459      4.6246\n",
      "  5.7477     2.6555    -1.3671    -3.7609    -1.4097     3.1587\n",
      " -2.2718    -1.6027     4.6539    -2.2436    -3.5011    -2.0355\n",
      "  3.1691    -1.1932     5.601      0.024695  -0.64505    0.31986\n",
      "  2.1783     3.4008    -0.46969    3.2835    -1.2504     0.12738\n",
      " -2.8327    -5.1578     0.25913   -0.49295    1.3293    -1.306\n",
      "  2.3585     1.527      5.3414    -1.2388     2.2665    -0.46179\n",
      " -1.1124     0.68584   -3.3001    -0.71354   -2.5839    -6.8384\n",
      "  0.99254    1.3082     8.0647     1.2911    -6.635      0.1252\n",
      "  2.2014    -1.4776    -4.9779     1.7243    -4.1845    -3.0842\n",
      "  2.2567     2.358     -2.4774     3.3282    -2.1753    -1.1066\n",
      " -3.7565     1.5521    -2.9964     0.084271   6.4939     2.3315\n",
      " -1.433      0.61267   -4.145     -0.22945    0.97798    4.1309\n",
      "  3.1957    -0.44863   -0.42043   -0.87894   -0.98122   -1.2932\n",
      "  5.3076    -1.2086    -2.1437    -2.7292    -4.5284     1.1712\n",
      " -1.2883     2.0682    -2.6284     3.4267     5.1492     3.09\n",
      " -5.62      -2.0655     5.0816     1.0811    -4.2311    -3.6099\n",
      " -4.9876    -0.067064  -4.8731    -2.1909     0.76246   -3.1653\n",
      " -0.76506    2.6255     1.4478    -3.8867     1.5705     1.4627\n",
      "  3.4962     0.86878    2.2051     2.4725    -4.209      4.6005\n",
      "  1.1193     1.8577    -0.35209   -6.5479     0.18243    0.27128\n",
      "  1.2043     2.5244    -4.8608     0.74402    1.6865    -1.7404\n",
      "  4.8251    -2.1395    -0.16218   -3.4141     2.5286     3.9907\n",
      "  2.9853    -0.61039   -2.5122     0.11862   -2.1776    -0.91608\n",
      "  1.8289     6.2185    -1.1455    -1.0649     0.6926     1.4772\n",
      "  5.3101    -1.867      4.8861    -1.8036     4.3231    -3.2203\n",
      " -2.0066    -3.1595    -1.7418     1.2043    -1.2297     3.3402\n",
      "  1.6516    -0.73984   -0.96135    2.2611    -2.3102    -2.3675\n",
      "  0.55139    1.3321     0.28221    0.93108    0.61875   -0.014785\n",
      " -0.94576    1.6944     3.6582    -0.27018    1.0145     5.1894\n",
      " -2.5645     2.0166     3.2151    -0.31021    0.79828    2.0354\n",
      " -1.9956    -3.5707    -0.61609   -4.8227    -8.5705     2.8654\n",
      "  3.6639     0.61012   -1.839     -3.9471    -4.9723     0.84935\n",
      "  0.04205    1.0362    -0.18908    4.6937    -3.9675     0.967\n",
      "  5.1043    -2.4924    -0.1602    -1.7642    -1.1315    -3.6023\n",
      " -0.45072    2.2964    -2.6973     3.1196     3.8481    -1.4449\n",
      "  0.25807    1.3712     1.847     -0.31741   -5.686     -1.6835   ]\n",
      "dog [  1.233       4.2963     -7.9738    -10.121       1.8207      1.4098\n",
      "  -4.518      -5.2261     -0.29157     0.95234     6.988       5.0637\n",
      "  -0.0055726   3.3395      6.4596     -6.3742      0.039045   -3.9855\n",
      "   1.2085     -1.3186     -4.8886      3.7066     -2.8281     -3.5447\n",
      "   0.76888     1.5016     -4.3632      8.648      -5.9286     -1.3055\n",
      "   0.8387      0.90137    -1.7843     -1.0148      2.73       -6.9039\n",
      "   0.80413     7.488       6.1078     -4.213      -0.15384    -5.4995\n",
      "  10.896       3.9278     -0.13601     0.077732    3.2218     -5.8777\n",
      "   0.61359    -2.4287      6.282      13.461       4.3236      2.4266\n",
      "  -2.6512      1.1577      5.0848     -1.7058      3.3824      3.285\n",
      "   1.0969     -8.3711     -1.5554      2.0296     -2.6796     -6.9195\n",
      "  -2.3386     -1.9916     -3.045       2.489       7.3247      1.3364\n",
      "   0.23828     0.084388    3.148      -1.1128     -3.5598     -0.12115\n",
      "  -2.0357     -3.2731     -7.7205      4.0948     -2.0732      2.0833\n",
      "  -2.2803     -4.985       9.7667      6.1779    -10.352      -2.2268\n",
      "   2.5765     -5.744       5.5564     -5.2735      3.0004     -4.2512\n",
      "  -1.5682      2.2698      1.0491     -9.0486      4.2936      1.8709\n",
      "   5.1985     -1.3153      6.5224      0.40113   -12.583       3.6534\n",
      "  -2.0961      1.0022     -1.7873     -4.2555      7.7471      1.0173\n",
      "   3.1626      2.3558      0.33589    -4.4178      5.0584     -2.4118\n",
      "  -2.7445      3.417     -11.574      -2.6568     -3.6933     -2.0398\n",
      "   5.0976      6.5249      3.3573      0.95334    -0.9443     -9.4395\n",
      "   2.7867     -1.7549      1.7287      3.4942     -1.6883     -3.5771\n",
      "  -1.9013      2.2239     -5.4335     -6.5724     -0.67228    -1.9748\n",
      "  -3.108      -1.857       0.99496     0.89135    -4.4254      0.33125\n",
      "   5.8815      1.9384      0.57294    -2.883       3.8087     -1.3095\n",
      "   5.9208      3.362       3.3571     -0.38807     0.90022    -5.5742\n",
      "  -4.2939      1.4992     -4.708      -2.9402     -1.2259      0.3098\n",
      "   1.8858     -1.9867     -0.23554    -0.54535    -0.21387     2.4797\n",
      "   5.971      -7.1249      1.6257     -1.5241      0.75974     1.4312\n",
      "   2.3641     -3.5566      0.92066     0.44934    -1.3233      3.1733\n",
      "  -4.7059    -12.09       -0.39241    -0.68457    -3.6789      6.6279\n",
      "  -2.9937     -3.8361      1.3868     -4.9002     -2.4299      6.4312\n",
      "   2.5056     -4.508      -5.1278     -1.5585     -3.0226     -0.86811\n",
      "  -1.1538     -1.0022     -0.91651    -0.4781     -1.6084     -2.7307\n",
      "   3.708       0.77423    -1.1085     -0.68755    -8.2901      3.2405\n",
      "  -0.16108    -0.62837    -5.596      -4.4865      0.40115    -3.7063\n",
      "  -2.1704      4.0789     -1.7973      8.9538      0.89421    -4.8128\n",
      "   4.5367     -0.32579    -5.2344     -3.9766     -2.1979      3.5699\n",
      "   1.4982      6.0972     -1.9704      4.6522     -0.37734     0.039101\n",
      "   2.5361     -1.8096      8.7035     -8.6372     -3.5257      3.1034\n",
      "   3.2635      4.5437     -5.729      -0.29141    -2.0011      8.5328\n",
      "  -4.5064     -4.8276    -11.786       0.35607    -5.7115      6.3122\n",
      "  -3.665       0.33597     2.5017     -3.5025     -3.7891     -3.1343\n",
      "  -1.4429     -6.9119     -2.6114     -0.59757     0.37847     6.3187\n",
      "   2.8965     -2.5397      1.8022      3.5486      4.4721     -4.8481\n",
      "  -3.6252      4.0969     -2.0081     -0.20122     2.5244     -0.68817\n",
      "   0.67184    -7.0466      1.6641     -2.2308     -3.896       6.132\n",
      "  -8.0335     -1.713       2.5688     -5.2547      6.9845      0.27835\n",
      "  -6.4554     -2.1327     -5.6515     11.174      -8.0568      5.7985   ]\n"
     ]
    }
   ],
   "source": [
    "# You can parse any sentence and get its word embeddings\n",
    "\n",
    "parsed_text = nlp(\"The green frog jumped the black dog\")\n",
    "\n",
    "glove_vectors = [w.vector for w in parsed_text]\n",
    "for i, w in enumerate(parsed_text):\n",
    "    print(w, glove_vectors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize GloVe word embeddings of our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>1.53330</td>\n",
       "      <td>1.622600</td>\n",
       "      <td>1.05520</td>\n",
       "      <td>-1.361500</td>\n",
       "      <td>0.97952</td>\n",
       "      <td>-0.828060</td>\n",
       "      <td>1.485400</td>\n",
       "      <td>2.881700</td>\n",
       "      <td>-0.354000</td>\n",
       "      <td>0.344570</td>\n",
       "      <td>...</td>\n",
       "      <td>3.243400</td>\n",
       "      <td>-1.53780</td>\n",
       "      <td>2.36610</td>\n",
       "      <td>-2.51120</td>\n",
       "      <td>-1.267200</td>\n",
       "      <td>-1.48080</td>\n",
       "      <td>-1.282500</td>\n",
       "      <td>1.41430</td>\n",
       "      <td>-2.50450</td>\n",
       "      <td>2.77440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>-3.84290</td>\n",
       "      <td>0.140680</td>\n",
       "      <td>-3.35840</td>\n",
       "      <td>3.267900</td>\n",
       "      <td>-2.15360</td>\n",
       "      <td>-6.508500</td>\n",
       "      <td>0.235120</td>\n",
       "      <td>6.884500</td>\n",
       "      <td>-0.543490</td>\n",
       "      <td>3.596900</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.592000</td>\n",
       "      <td>1.02690</td>\n",
       "      <td>1.60640</td>\n",
       "      <td>-2.84040</td>\n",
       "      <td>-2.274700</td>\n",
       "      <td>2.10020</td>\n",
       "      <td>5.480200</td>\n",
       "      <td>0.83172</td>\n",
       "      <td>-4.30830</td>\n",
       "      <td>-1.04370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>-1.87590</td>\n",
       "      <td>-3.145100</td>\n",
       "      <td>0.26432</td>\n",
       "      <td>2.512800</td>\n",
       "      <td>5.12880</td>\n",
       "      <td>1.515500</td>\n",
       "      <td>-3.733400</td>\n",
       "      <td>3.628500</td>\n",
       "      <td>-0.975220</td>\n",
       "      <td>-1.097900</td>\n",
       "      <td>...</td>\n",
       "      <td>5.413500</td>\n",
       "      <td>0.45700</td>\n",
       "      <td>-3.24810</td>\n",
       "      <td>-1.46680</td>\n",
       "      <td>0.405470</td>\n",
       "      <td>3.04020</td>\n",
       "      <td>-1.674500</td>\n",
       "      <td>-3.53990</td>\n",
       "      <td>-4.88900</td>\n",
       "      <td>-2.04060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>-0.68660</td>\n",
       "      <td>-1.726700</td>\n",
       "      <td>-3.00130</td>\n",
       "      <td>-1.110100</td>\n",
       "      <td>1.83890</td>\n",
       "      <td>-3.076600</td>\n",
       "      <td>-0.233690</td>\n",
       "      <td>1.013300</td>\n",
       "      <td>-2.023400</td>\n",
       "      <td>3.033700</td>\n",
       "      <td>...</td>\n",
       "      <td>1.173600</td>\n",
       "      <td>-2.72720</td>\n",
       "      <td>1.57230</td>\n",
       "      <td>-2.42900</td>\n",
       "      <td>-1.527600</td>\n",
       "      <td>0.98387</td>\n",
       "      <td>-0.098775</td>\n",
       "      <td>2.95160</td>\n",
       "      <td>-1.60790</td>\n",
       "      <td>2.54120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>2.05650</td>\n",
       "      <td>-3.225900</td>\n",
       "      <td>-5.73640</td>\n",
       "      <td>-6.146000</td>\n",
       "      <td>0.15748</td>\n",
       "      <td>-2.428400</td>\n",
       "      <td>7.658000</td>\n",
       "      <td>2.706400</td>\n",
       "      <td>-2.211000</td>\n",
       "      <td>-0.899900</td>\n",
       "      <td>...</td>\n",
       "      <td>1.580200</td>\n",
       "      <td>1.75970</td>\n",
       "      <td>-0.60806</td>\n",
       "      <td>-6.61070</td>\n",
       "      <td>0.009383</td>\n",
       "      <td>-4.27630</td>\n",
       "      <td>-0.505070</td>\n",
       "      <td>5.00490</td>\n",
       "      <td>-8.53120</td>\n",
       "      <td>-1.49670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-1.58880</td>\n",
       "      <td>0.733160</td>\n",
       "      <td>1.07460</td>\n",
       "      <td>-2.452100</td>\n",
       "      <td>-0.42517</td>\n",
       "      <td>3.334400</td>\n",
       "      <td>-0.179420</td>\n",
       "      <td>-1.092400</td>\n",
       "      <td>-0.093904</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125300</td>\n",
       "      <td>-2.35530</td>\n",
       "      <td>3.93600</td>\n",
       "      <td>-3.37510</td>\n",
       "      <td>-0.999340</td>\n",
       "      <td>1.26390</td>\n",
       "      <td>-2.106800</td>\n",
       "      <td>3.07430</td>\n",
       "      <td>-4.42900</td>\n",
       "      <td>3.89380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>0.93585</td>\n",
       "      <td>-0.450810</td>\n",
       "      <td>-4.95600</td>\n",
       "      <td>2.254800</td>\n",
       "      <td>1.21050</td>\n",
       "      <td>-1.667600</td>\n",
       "      <td>0.879590</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>-4.167800</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.116600</td>\n",
       "      <td>1.10070</td>\n",
       "      <td>4.04960</td>\n",
       "      <td>-0.97496</td>\n",
       "      <td>-5.127300</td>\n",
       "      <td>-0.63158</td>\n",
       "      <td>6.890100</td>\n",
       "      <td>-0.34744</td>\n",
       "      <td>2.44190</td>\n",
       "      <td>0.90073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>7.15240</td>\n",
       "      <td>3.035600</td>\n",
       "      <td>-8.70400</td>\n",
       "      <td>0.937880</td>\n",
       "      <td>-3.67810</td>\n",
       "      <td>-0.035128</td>\n",
       "      <td>1.256800</td>\n",
       "      <td>0.024432</td>\n",
       "      <td>0.289240</td>\n",
       "      <td>4.505400</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.771200</td>\n",
       "      <td>4.76440</td>\n",
       "      <td>4.23500</td>\n",
       "      <td>-1.84270</td>\n",
       "      <td>0.369470</td>\n",
       "      <td>1.31650</td>\n",
       "      <td>1.165000</td>\n",
       "      <td>-1.39280</td>\n",
       "      <td>-3.91250</td>\n",
       "      <td>0.48484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>-0.44031</td>\n",
       "      <td>-2.975000</td>\n",
       "      <td>-1.22630</td>\n",
       "      <td>1.393000</td>\n",
       "      <td>2.91210</td>\n",
       "      <td>-2.842100</td>\n",
       "      <td>-0.390080</td>\n",
       "      <td>0.122480</td>\n",
       "      <td>-2.892300</td>\n",
       "      <td>1.735200</td>\n",
       "      <td>...</td>\n",
       "      <td>3.720400</td>\n",
       "      <td>-1.21420</td>\n",
       "      <td>3.37650</td>\n",
       "      <td>-4.15160</td>\n",
       "      <td>-0.408940</td>\n",
       "      <td>-0.13075</td>\n",
       "      <td>1.939700</td>\n",
       "      <td>2.75650</td>\n",
       "      <td>1.44580</td>\n",
       "      <td>1.57040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.44278</td>\n",
       "      <td>-4.114100</td>\n",
       "      <td>1.23560</td>\n",
       "      <td>4.187300</td>\n",
       "      <td>1.62440</td>\n",
       "      <td>-1.260100</td>\n",
       "      <td>-1.643500</td>\n",
       "      <td>-0.900330</td>\n",
       "      <td>-1.241500</td>\n",
       "      <td>0.867240</td>\n",
       "      <td>...</td>\n",
       "      <td>3.657400</td>\n",
       "      <td>-1.63910</td>\n",
       "      <td>3.90300</td>\n",
       "      <td>-6.56120</td>\n",
       "      <td>3.673700</td>\n",
       "      <td>-1.20580</td>\n",
       "      <td>3.819300</td>\n",
       "      <td>3.60500</td>\n",
       "      <td>3.63860</td>\n",
       "      <td>4.03620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>2.04220</td>\n",
       "      <td>3.634600</td>\n",
       "      <td>0.49222</td>\n",
       "      <td>0.074497</td>\n",
       "      <td>1.18540</td>\n",
       "      <td>2.842600</td>\n",
       "      <td>-0.042565</td>\n",
       "      <td>5.634800</td>\n",
       "      <td>0.948340</td>\n",
       "      <td>0.402610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538160</td>\n",
       "      <td>3.18240</td>\n",
       "      <td>1.99210</td>\n",
       "      <td>-1.43010</td>\n",
       "      <td>-0.164580</td>\n",
       "      <td>2.53300</td>\n",
       "      <td>-0.051668</td>\n",
       "      <td>1.00780</td>\n",
       "      <td>-0.29046</td>\n",
       "      <td>-1.68560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>0.56940</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>-2.79580</td>\n",
       "      <td>-6.990900</td>\n",
       "      <td>-1.94450</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>-4.308900</td>\n",
       "      <td>-2.577500</td>\n",
       "      <td>3.211000</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050673</td>\n",
       "      <td>1.79860</td>\n",
       "      <td>6.77940</td>\n",
       "      <td>-2.54120</td>\n",
       "      <td>-0.408370</td>\n",
       "      <td>3.17150</td>\n",
       "      <td>0.974170</td>\n",
       "      <td>5.32660</td>\n",
       "      <td>-10.14900</td>\n",
       "      <td>-0.31148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>0.22776</td>\n",
       "      <td>-3.051600</td>\n",
       "      <td>0.26025</td>\n",
       "      <td>0.641570</td>\n",
       "      <td>2.04230</td>\n",
       "      <td>-3.136500</td>\n",
       "      <td>0.128790</td>\n",
       "      <td>0.181220</td>\n",
       "      <td>-1.535200</td>\n",
       "      <td>0.290250</td>\n",
       "      <td>...</td>\n",
       "      <td>3.504000</td>\n",
       "      <td>-1.27140</td>\n",
       "      <td>4.07630</td>\n",
       "      <td>-5.66470</td>\n",
       "      <td>0.247550</td>\n",
       "      <td>1.58200</td>\n",
       "      <td>3.224800</td>\n",
       "      <td>3.00600</td>\n",
       "      <td>2.78730</td>\n",
       "      <td>2.65260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-4.31020</td>\n",
       "      <td>2.570600</td>\n",
       "      <td>-3.47220</td>\n",
       "      <td>2.520500</td>\n",
       "      <td>-1.11040</td>\n",
       "      <td>-5.101400</td>\n",
       "      <td>-1.481200</td>\n",
       "      <td>4.655900</td>\n",
       "      <td>0.383650</td>\n",
       "      <td>2.477200</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.250500</td>\n",
       "      <td>4.20530</td>\n",
       "      <td>2.92490</td>\n",
       "      <td>-0.93310</td>\n",
       "      <td>0.890710</td>\n",
       "      <td>6.85240</td>\n",
       "      <td>0.075678</td>\n",
       "      <td>1.07370</td>\n",
       "      <td>-2.96000</td>\n",
       "      <td>-0.42209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-1.93390</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>-4.39510</td>\n",
       "      <td>1.924700</td>\n",
       "      <td>-2.04800</td>\n",
       "      <td>-5.712100</td>\n",
       "      <td>-0.956200</td>\n",
       "      <td>2.792700</td>\n",
       "      <td>-1.307100</td>\n",
       "      <td>3.660400</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.795100</td>\n",
       "      <td>0.43909</td>\n",
       "      <td>2.74080</td>\n",
       "      <td>-2.18080</td>\n",
       "      <td>-0.936220</td>\n",
       "      <td>1.71610</td>\n",
       "      <td>4.210300</td>\n",
       "      <td>-0.44140</td>\n",
       "      <td>-3.99350</td>\n",
       "      <td>-0.57955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.94699</td>\n",
       "      <td>-3.802500</td>\n",
       "      <td>-5.22820</td>\n",
       "      <td>1.638600</td>\n",
       "      <td>4.74710</td>\n",
       "      <td>-3.459600</td>\n",
       "      <td>-2.239200</td>\n",
       "      <td>0.847490</td>\n",
       "      <td>-3.645700</td>\n",
       "      <td>3.273800</td>\n",
       "      <td>...</td>\n",
       "      <td>6.575500</td>\n",
       "      <td>1.13930</td>\n",
       "      <td>-2.46300</td>\n",
       "      <td>-7.98480</td>\n",
       "      <td>-7.093400</td>\n",
       "      <td>-2.87130</td>\n",
       "      <td>3.126500</td>\n",
       "      <td>3.40870</td>\n",
       "      <td>-4.73420</td>\n",
       "      <td>-0.91956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1.23300</td>\n",
       "      <td>4.296300</td>\n",
       "      <td>-7.97380</td>\n",
       "      <td>-10.121000</td>\n",
       "      <td>1.82070</td>\n",
       "      <td>1.409800</td>\n",
       "      <td>-4.518000</td>\n",
       "      <td>-5.226100</td>\n",
       "      <td>-0.291570</td>\n",
       "      <td>0.952340</td>\n",
       "      <td>...</td>\n",
       "      <td>2.568800</td>\n",
       "      <td>-5.25470</td>\n",
       "      <td>6.98450</td>\n",
       "      <td>0.27835</td>\n",
       "      <td>-6.455400</td>\n",
       "      <td>-2.13270</td>\n",
       "      <td>-5.651500</td>\n",
       "      <td>11.17400</td>\n",
       "      <td>-8.05680</td>\n",
       "      <td>5.79850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>-1.93140</td>\n",
       "      <td>0.078516</td>\n",
       "      <td>1.47560</td>\n",
       "      <td>1.597600</td>\n",
       "      <td>1.57150</td>\n",
       "      <td>-2.340500</td>\n",
       "      <td>0.164920</td>\n",
       "      <td>0.763640</td>\n",
       "      <td>-1.061700</td>\n",
       "      <td>3.975100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873200</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>-1.02720</td>\n",
       "      <td>-0.16442</td>\n",
       "      <td>-1.481600</td>\n",
       "      <td>-1.98390</td>\n",
       "      <td>1.898100</td>\n",
       "      <td>2.54110</td>\n",
       "      <td>2.28320</td>\n",
       "      <td>1.28470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>2.63220</td>\n",
       "      <td>2.707800</td>\n",
       "      <td>-1.19830</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>3.34010</td>\n",
       "      <td>-2.768000</td>\n",
       "      <td>0.285130</td>\n",
       "      <td>0.959040</td>\n",
       "      <td>-3.211000</td>\n",
       "      <td>0.978550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742100</td>\n",
       "      <td>-0.82167</td>\n",
       "      <td>2.79760</td>\n",
       "      <td>0.21357</td>\n",
       "      <td>1.293800</td>\n",
       "      <td>0.10757</td>\n",
       "      <td>0.832680</td>\n",
       "      <td>-0.80778</td>\n",
       "      <td>-1.53100</td>\n",
       "      <td>-0.28953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-0.18931</td>\n",
       "      <td>-0.706820</td>\n",
       "      <td>-3.17310</td>\n",
       "      <td>-4.140700</td>\n",
       "      <td>0.70243</td>\n",
       "      <td>-0.692550</td>\n",
       "      <td>2.348700</td>\n",
       "      <td>1.708300</td>\n",
       "      <td>-0.927380</td>\n",
       "      <td>3.484300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594240</td>\n",
       "      <td>2.14920</td>\n",
       "      <td>3.27380</td>\n",
       "      <td>-1.28600</td>\n",
       "      <td>1.950500</td>\n",
       "      <td>-0.67481</td>\n",
       "      <td>-0.930960</td>\n",
       "      <td>3.28140</td>\n",
       "      <td>-5.52320</td>\n",
       "      <td>-0.31346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1        2          3        4         5         6    \\\n",
       "today      1.53330  1.622600  1.05520  -1.361500  0.97952 -0.828060  1.485400   \n",
       "brown     -3.84290  0.140680 -3.35840   3.267900 -2.15360 -6.508500  0.235120   \n",
       "kings     -1.87590 -3.145100  0.26432   2.512800  5.12880  1.515500 -3.733400   \n",
       "breakfast -0.68660 -1.726700 -3.00130  -1.110100  1.83890 -3.076600 -0.233690   \n",
       "love       2.05650 -3.225900 -5.73640  -6.146000  0.15748 -2.428400  7.658000   \n",
       "lazy      -1.58880  0.733160  1.07460  -2.452100 -0.42517  3.334400 -0.179420   \n",
       "beans      0.93585 -0.450810 -4.95600   2.254800  1.21050 -1.667600  0.879590   \n",
       "sky        7.15240  3.035600 -8.70400   0.937880 -3.67810 -0.035128  1.256800   \n",
       "sausages  -0.44031 -2.975000 -1.22630   1.393000  2.91210 -2.842100 -0.390080   \n",
       "ham        0.44278 -4.114100  1.23560   4.187300  1.62440 -1.260100 -1.643500   \n",
       "jumps      2.04220  3.634600  0.49222   0.074497  1.18540  2.842600 -0.042565   \n",
       "fox        0.56940  1.003500 -2.79580  -6.990900 -1.94450  2.147300 -4.308900   \n",
       "bacon      0.22776 -3.051600  0.26025   0.641570  2.04230 -3.136500  0.128790   \n",
       "blue      -4.31020  2.570600 -3.47220   2.520500 -1.11040 -5.101400 -1.481200   \n",
       "green     -1.93390  0.314870 -4.39510   1.924700 -2.04800 -5.712100 -0.956200   \n",
       "eggs       0.94699 -3.802500 -5.22820   1.638600  4.74710 -3.459600 -2.239200   \n",
       "dog        1.23300  4.296300 -7.97380 -10.121000  1.82070  1.409800 -4.518000   \n",
       "toast     -1.93140  0.078516  1.47560   1.597600  1.57150 -2.340500  0.164920   \n",
       "quick      2.63220  2.707800 -1.19830   2.149000  3.34010 -2.768000  0.285130   \n",
       "beautiful -0.18931 -0.706820 -3.17310  -4.140700  0.70243 -0.692550  2.348700   \n",
       "\n",
       "                7         8         9    ...       290      291      292  \\\n",
       "today      2.881700 -0.354000  0.344570  ...  3.243400 -1.53780  2.36610   \n",
       "brown      6.884500 -0.543490  3.596900  ... -1.592000  1.02690  1.60640   \n",
       "kings      3.628500 -0.975220 -1.097900  ...  5.413500  0.45700 -3.24810   \n",
       "breakfast  1.013300 -2.023400  3.033700  ...  1.173600 -2.72720  1.57230   \n",
       "love       2.706400 -2.211000 -0.899900  ...  1.580200  1.75970 -0.60806   \n",
       "lazy      -1.092400 -0.093904  0.058663  ...  1.125300 -2.35530  3.93600   \n",
       "beans      0.004286 -4.167800  3.960000  ...  3.116600  1.10070  4.04960   \n",
       "sky        0.024432  0.289240  4.505400  ... -5.771200  4.76440  4.23500   \n",
       "sausages   0.122480 -2.892300  1.735200  ...  3.720400 -1.21420  3.37650   \n",
       "ham       -0.900330 -1.241500  0.867240  ...  3.657400 -1.63910  3.90300   \n",
       "jumps      5.634800  0.948340  0.402610  ... -0.538160  3.18240  1.99210   \n",
       "fox       -2.577500  3.211000  0.202600  ... -0.050673  1.79860  6.77940   \n",
       "bacon      0.181220 -1.535200  0.290250  ...  3.504000 -1.27140  4.07630   \n",
       "blue       4.655900  0.383650  2.477200  ... -5.250500  4.20530  2.92490   \n",
       "green      2.792700 -1.307100  3.660400  ... -2.795100  0.43909  2.74080   \n",
       "eggs       0.847490 -3.645700  3.273800  ...  6.575500  1.13930 -2.46300   \n",
       "dog       -5.226100 -0.291570  0.952340  ...  2.568800 -5.25470  6.98450   \n",
       "toast      0.763640 -1.061700  3.975100  ...  1.873200  0.96658 -1.02720   \n",
       "quick      0.959040 -3.211000  0.978550  ...  1.742100 -0.82167  2.79760   \n",
       "beautiful  1.708300 -0.927380  3.484300  ...  0.594240  2.14920  3.27380   \n",
       "\n",
       "               293       294      295       296       297       298      299  \n",
       "today     -2.51120 -1.267200 -1.48080 -1.282500   1.41430  -2.50450  2.77440  \n",
       "brown     -2.84040 -2.274700  2.10020  5.480200   0.83172  -4.30830 -1.04370  \n",
       "kings     -1.46680  0.405470  3.04020 -1.674500  -3.53990  -4.88900 -2.04060  \n",
       "breakfast -2.42900 -1.527600  0.98387 -0.098775   2.95160  -1.60790  2.54120  \n",
       "love      -6.61070  0.009383 -4.27630 -0.505070   5.00490  -8.53120 -1.49670  \n",
       "lazy      -3.37510 -0.999340  1.26390 -2.106800   3.07430  -4.42900  3.89380  \n",
       "beans     -0.97496 -5.127300 -0.63158  6.890100  -0.34744   2.44190  0.90073  \n",
       "sky       -1.84270  0.369470  1.31650  1.165000  -1.39280  -3.91250  0.48484  \n",
       "sausages  -4.15160 -0.408940 -0.13075  1.939700   2.75650   1.44580  1.57040  \n",
       "ham       -6.56120  3.673700 -1.20580  3.819300   3.60500   3.63860  4.03620  \n",
       "jumps     -1.43010 -0.164580  2.53300 -0.051668   1.00780  -0.29046 -1.68560  \n",
       "fox       -2.54120 -0.408370  3.17150  0.974170   5.32660 -10.14900 -0.31148  \n",
       "bacon     -5.66470  0.247550  1.58200  3.224800   3.00600   2.78730  2.65260  \n",
       "blue      -0.93310  0.890710  6.85240  0.075678   1.07370  -2.96000 -0.42209  \n",
       "green     -2.18080 -0.936220  1.71610  4.210300  -0.44140  -3.99350 -0.57955  \n",
       "eggs      -7.98480 -7.093400 -2.87130  3.126500   3.40870  -4.73420 -0.91956  \n",
       "dog        0.27835 -6.455400 -2.13270 -5.651500  11.17400  -8.05680  5.79850  \n",
       "toast     -0.16442 -1.481600 -1.98390  1.898100   2.54110   2.28320  1.28470  \n",
       "quick      0.21357  1.293800  0.10757  0.832680  -0.80778  -1.53100 -0.28953  \n",
       "beautiful -1.28600  1.950500 -0.67481 -0.930960   3.28140  -5.52320 -0.31346  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set([word for sublist in [doc.split() for doc in norm_corpus] for word in sublist]))\n",
    "\n",
    "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
    "pd.DataFrame(word_glove_vectors, index=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afalcao/miniconda3/envs/mo434/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_glove_vectors)\n",
    "labels = unique_words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster documents with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document  \\\n",
       "0                                      The sky is blue and beautiful.   \n",
       "1                                   Love this blue and beautiful sky!   \n",
       "2                        The quick brown fox jumps over the lazy dog.   \n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans   \n",
       "4                         I love green eggs, ham, sausages and bacon!   \n",
       "5                    The brown fox is quick and the blue dog is lazy!   \n",
       "6            The sky is very blue and the sky is very beautiful today   \n",
       "7                         The dog is lazy but the brown fox is quick!   \n",
       "\n",
       "  Category  ClusterLabel  \n",
       "0  weather             1  \n",
       "1  weather             1  \n",
       "2  animals             2  \n",
       "3     food             0  \n",
       "4     food             0  \n",
       "5  animals             2  \n",
       "6  weather             1  \n",
       "7  animals             2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "doc_glove_vectors = np.array([nlp(str(doc)).vector for doc in norm_corpus])\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "km.fit_transform(doc_glove_vectors)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging gensim for building a FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText \n",
    "\n",
    "\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_bible]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size   = 300 # Word vector dimensionality  \n",
    "window_context = 30  # Context window size                                                                                    \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_cores      = 12  # Number of processors\n",
    "\n",
    "# train the model \n",
    "fast_text = FastText(tokenized_corpus, vector_size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count, workers=num_cores, epochs=50) # it does not support n-grams with n\n",
    "                                                                            # different than 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'god': ['godly', 'goddess', 'gods', 'godliness', 'lord'],\n",
       " 'jesus': ['ephesus', 'tarsus', 'gaius', 'alphaeus', 'scripture'],\n",
       " 'noah': ['zanoah', 'manoah', 'adah', 'milcah', 'joah'],\n",
       " 'egypt': ['egyptian', 'egyptians', 'pharaoh', 'bondage', 'canaan'],\n",
       " 'john': ['alphaeus', 'galilee', 'herodias', 'james', 'baptist'],\n",
       " 'gospel': ['christ', 'hope', 'superscription', 'grace', 'revelation'],\n",
       " 'moses': ['joses', 'moza', 'purposes', 'amos', 'asses'],\n",
       " 'famine': ['examine', 'familiar', 'family', 'mine', 'pestilence']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view similar words based on the fast_text model\n",
    "similar_words = {search_term: [item[0] for item in fast_text.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize their projection by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = fast_text.wv[words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "P = pca.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.scatter(P[:, 0], P[:, 1], c='lightgreen', edgecolors='g')\n",
    "for label, x, y in zip(labels, P[:, 0], P[:, 1]):\n",
    "    plt.annotate(label, xy=(x+0.06, y+0.03), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30584332,  1.4597086 , -0.20075062, -3.5213044 , -3.5032415 ,\n",
       "        0.6045214 , -0.7558142 ,  1.6340855 , -0.7819558 ,  0.46100932,\n",
       "       -2.3164396 ,  2.398333  , -0.56278855, -1.6978656 , -1.9863446 ,\n",
       "       -2.5130289 ,  2.6174915 , -1.2601485 ,  3.0547707 , -0.15412554,\n",
       "       -2.0985327 ,  2.2201421 , -3.5260022 , -3.184709  ,  0.37583095,\n",
       "        0.40446222, -0.14917588,  0.40501443,  1.0694941 , -0.06712214,\n",
       "        1.8951794 , -1.4284976 ,  1.948012  ,  0.04004262, -0.53943264,\n",
       "        0.8933363 ,  0.6302208 ,  0.87466747, -0.16572584, -0.37349617,\n",
       "        1.0459887 , -2.779037  ,  2.5061605 ,  0.7256071 ,  2.5760088 ,\n",
       "       -1.5487748 , -3.5315645 ,  4.838411  , -3.289885  , -1.5516857 ,\n",
       "        0.22165133, -1.4997333 ,  0.5478979 ,  0.86551076, -1.4852788 ,\n",
       "        0.30736035,  1.2807195 ,  0.9958289 ,  1.9986396 , -3.071913  ,\n",
       "       -1.1619956 , -2.427179  ,  0.04490274, -1.311942  , -2.5709636 ,\n",
       "        0.5152984 ,  1.3082554 , -0.5259133 , -1.1672891 ,  0.78802514,\n",
       "       -1.8966371 , -1.480961  , -0.44210702, -0.63186485,  0.7416883 ,\n",
       "        2.100936  ,  4.990377  , -0.58414066, -0.8288823 ,  0.5475496 ,\n",
       "        0.65412027,  1.4856993 ,  0.8156146 , -2.051395  ,  1.3974828 ,\n",
       "       -2.2474058 ,  2.0538163 ,  1.3253057 ,  2.3465233 ,  1.610415  ,\n",
       "       -0.2487243 ,  0.9510323 , -0.9927522 ,  0.00634501,  3.0062766 ,\n",
       "        1.4338695 ,  2.6928213 , -1.0387162 ,  3.4807703 ,  1.3541456 ,\n",
       "       -0.7799563 , -0.7010527 ,  0.6032906 , -1.7152438 , -0.701394  ,\n",
       "        0.5462913 ,  1.2998118 , -0.8024407 ,  0.7142985 ,  1.5438914 ,\n",
       "        0.31460494, -1.5981116 ,  0.49617332,  0.38780087,  0.13260013,\n",
       "        0.10187577, -0.49481487, -1.4289657 ,  0.38896865, -1.6454654 ,\n",
       "        2.2276697 ,  2.8904045 ,  0.30989063, -2.102562  ,  0.6041897 ,\n",
       "        1.6502784 , -2.6743312 ,  1.1466357 , -0.2941536 , -0.11988066,\n",
       "        1.1210163 , -0.24161434,  0.8202212 , -5.1458077 , -0.24483795,\n",
       "       -2.3896441 ,  0.5414404 , -0.72516274, -0.8824135 ,  0.07027658,\n",
       "       -3.2771926 , -0.22578187,  2.9558923 , -0.6185271 , -1.2146571 ,\n",
       "        1.545575  ,  0.48581797,  0.4169126 , -0.3800527 ,  0.2140045 ,\n",
       "        1.1954545 , -0.5020526 , -3.8785584 , -0.63585865,  0.79462034,\n",
       "        2.3587687 , -4.205103  , -1.5300319 , -1.1822045 ,  1.0309556 ,\n",
       "       -2.954901  , -0.956261  , -0.38289914,  1.6548393 , -0.3288081 ,\n",
       "        2.6566596 ,  0.11215269, -2.1965816 ,  1.6068214 ,  0.26312962,\n",
       "        2.7731154 , -0.80768996, -0.3702081 ,  4.108777  , -1.2498382 ,\n",
       "        1.1536738 , -0.11890473, -2.113019  , -1.1820552 , -1.411509  ,\n",
       "        2.1455016 , -3.7838979 , -0.7273609 ,  1.2225024 ,  2.6109078 ,\n",
       "       -2.2318976 ,  2.1193101 , -0.75324667, -1.4606011 , -0.39373386,\n",
       "       -0.78262645, -0.6691199 ,  0.2877367 , -0.38400754,  0.2968402 ,\n",
       "       -1.5072657 ,  4.6057806 ,  0.42270428,  0.27554643,  1.4556518 ,\n",
       "        0.09720019,  1.0758942 ,  2.1435149 , -0.24546191,  0.775544  ,\n",
       "        2.430498  ,  1.6803054 ,  3.4418356 ,  1.5062864 ,  0.9915803 ,\n",
       "        0.8690052 , -0.20941582, -0.669923  ,  1.2897925 ,  0.8457142 ,\n",
       "        2.267101  ,  3.5125601 ,  0.66557497, -1.1948411 ,  0.87773335,\n",
       "        0.28195602,  1.5602227 , -0.7815713 ,  2.2657886 , -0.9651572 ,\n",
       "       -2.1246438 ,  1.3966527 ,  0.18405613,  2.9726424 ,  3.5900285 ,\n",
       "        1.6746849 , -0.11889348, -0.93561226,  0.27262327, -2.6460652 ,\n",
       "       -1.4990042 , -2.1586337 , -1.3787559 , -1.0939869 , -1.3911126 ,\n",
       "       -0.09271857,  1.50369   ,  0.21174106, -1.5967225 ,  0.7931127 ,\n",
       "        2.7562256 , -2.5544763 , -0.5795925 , -1.7446294 ,  1.1387866 ,\n",
       "        2.5227737 ,  0.5866334 , -1.6389304 , -1.2179486 , -1.969297  ,\n",
       "        0.1329559 ,  2.3504846 , -0.13863479, -0.5041818 ,  1.4907216 ,\n",
       "       -2.2352104 , -0.71069986,  1.7462313 , -1.4650854 , -1.0729767 ,\n",
       "        0.72385776,  4.8251853 , -1.1030042 , -0.07941333,  2.9344363 ,\n",
       "       -0.7878739 ,  0.18370324, -0.04498429,  1.7166578 ,  2.3436377 ,\n",
       "        0.9147471 ,  0.19288352, -3.7549734 , -0.733269  ,  1.1208665 ,\n",
       "       -1.0796888 , -0.2710829 ,  2.6634014 , -0.08241407,  1.5653015 ,\n",
       "       -2.3048651 ,  1.1627924 , -0.71249807, -2.7625422 , -2.8800547 ,\n",
       "        4.422421  , -0.37531856,  0.52487755, -0.32777074,  2.1042001 ,\n",
       "       -2.4002035 ,  0.9210295 ,  3.9418402 ,  2.673491  ,  0.6173595 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text.wv['jesus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011600196\n",
      "0.11037772\n"
     ]
    }
   ],
   "source": [
    "print(fast_text.wv.similarity(w1='god', w2='satan'))\n",
    "print(fast_text.wv.similarity(w1='god', w2='jesus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd one out for [ christ jesus satan gospel ]: satan\n",
      "Odd one out for [ john peter james judas ]: judas\n"
     ]
    }
   ],
   "source": [
    "st1 = \"christ jesus satan gospel\"\n",
    "print('Odd one out for [',st1, ']:', fast_text.wv.doesnt_match(st1.split()))\n",
    "\n",
    "st2 = \"john peter james judas\"\n",
    "print('Odd one out for [',st2, ']:', fast_text.wv.doesnt_match(st2.split()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
